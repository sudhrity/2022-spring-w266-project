{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sUPGDF4eEVyL"
   },
   "outputs": [],
   "source": [
    "# # Import packages\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from os import getcwd\n",
    "import random\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract the NEs\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in /home/ubuntu/anaconda3/lib/python3.8/site-packages (0.2.8)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (4.6.3)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (8.2.0)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (2.8.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (5.4.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (6.0.8)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (2.25.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (3.6.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.2.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (4.62.3)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (2021.4.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFnDpMgHt1ug",
    "outputId": "6b4e7808-cc25-4a17-f61f-8b52be480dfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2RLP9n430teL"
   },
   "outputs": [],
   "source": [
    "#headlines_processed = []\n",
    "#clean_news = []\n",
    "text = []\n",
    "title = []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qk6OnnphsEeZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/w266-finalproject'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path_covid_dataset = './dataset/16119_db21c91a1ab47385bb13773ed8238c31'\n",
    "path_cnn_downloads = './dataset/cnn/downloads'\n",
    "path_models = './models/'\n",
    "path_outputs = './outputs/'\n",
    "path_processed_data= './processed_data/'\n",
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "#    text = text.lower() # lowercase\n",
    "    text = text.split() # convert have'nt -> have not\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_mapping:\n",
    "            text[i] = contraction_mapping[word]\n",
    "    #text = \" \".join(text)\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# device = 0, cuda\n",
    "#nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0)\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    " \n",
    "def remove_duplicates(input):\n",
    " \n",
    "    # split input string separated by space\n",
    "    input = input.split(\" \")\n",
    " \n",
    "    # now create dictionary using counter method\n",
    "    # which will have strings as key and their\n",
    "    # frequencies as value\n",
    "    unique_words = Counter(input)\n",
    " \n",
    "    # joins two adjacent elements in iterable way\n",
    "    s = \" \".join(unique_words.keys())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = \"My name is Wolfgang and I live in Berlin, Saratoga, December 2019, Washington, Bill, Delhi, Modi Putin \"\n",
    "\n",
    "# ner_results = nlp(example)\n",
    "# print(ner_results)\n",
    "\n",
    "\n",
    "#for item in X_test:\n",
    "    \n",
    "def get_entities(article):    \n",
    "    \n",
    "    entities = []\n",
    "    ner = []\n",
    "    ner_results = nlp_ner(article)\n",
    "    ner.append(ner_results)\n",
    "    nes = []\n",
    "    for ne in range(len(ner_results)):\n",
    "        if ner_results[ne]['entity'] == 'B-PER' or ner_results[ne]['entity'] == 'I-PER' or ner_results[ne]['entity'] == 'B-LOC' or ner_results[ne]['entity'] == 'I-LOC' or ner_results[ne]['entity'] == 'I-ORG' or ner_results[ne]['entity'] == 'B-ORG' or ner_results[ne]['entity'] == 'I-MISC':\n",
    "            nes.append(ner_results[ne]['word'])\n",
    "\n",
    "    if len(nes) == 0:\n",
    "        nes.append('')\n",
    "    \n",
    "    named_entities = remove_duplicates(' '.join(nes).replace(\" ##\", \"\").replace(\"##\", \"\"))\n",
    "    #named_entities = re.sub(r'\\ .* \\ ', ' ', named_entities)\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"My name is Wolfgang and I live in Berlin, Saratoga. Community December 2019. Washington, Bill. Delhi, Modi Putin \"\n",
    "\n",
    "# get_entities(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_text(article, entities, max_sentence_percent):\n",
    "\n",
    "    #print(entities)\n",
    "    entities_array = entities.split()\n",
    "    # returns a string with ner sentences\n",
    "\n",
    "    sentences_array = []\n",
    "    ner_sentences = []\n",
    "\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    #print(sentences)\n",
    "    sentence_dict = {}\n",
    "    sentence_order_dict = {}\n",
    "    for k, sentence in enumerate(sentences):\n",
    "        sentence_order_dict[sentence] = k\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    #print(\"sentence count:\", sentence_count)\n",
    "    \n",
    "    max_sentence_count = max(1, int(sentence_count * max_sentence_percent))\n",
    "    #for sentence in sentences:\n",
    "    for k, sentence in enumerate(sentences): \n",
    "        count = 0\n",
    "        for word in entities_array:\n",
    "            if word in sentence:\n",
    "                count += 1\n",
    "        #print(count)\n",
    "        sentence_dict[sentence] = count\n",
    "    \n",
    "    sorted_sd = sorted(sentence_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    for item in sorted_sd:\n",
    "        ner_sentences.append(''.join(item[0]))\n",
    "        #print(item[0], item[1])\n",
    "    \n",
    "    sentence_dict = {}\n",
    "                \n",
    "    for sentence in ner_sentences[:max_sentence_count]:\n",
    "        sentence_dict[sentence] = sentence_order_dict[sentence]\n",
    "        \n",
    "    sorted_sd = sorted(sentence_dict.items(), key=operator.itemgetter(1), reverse=False)\n",
    "\n",
    "    ner_sentences = []\n",
    "    for item in sorted_sd:\n",
    "        ner_sentences.append(''.join(item[0]))\n",
    "\n",
    "    ner_sentences = ' '.join(ner_sentences)\n",
    "            \n",
    "    if len(ner_sentences) == 0:\n",
    "        for item in sentences:\n",
    "            len_sentences = min(max_sentence_count, len(sentences))\n",
    "            ner_sentences = ' '.join(sentences[:len_sentences])\n",
    "    \n",
    "    return ner_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences = nltk.sent_tokenize(example)\n",
    "# len_sentences = min(3, len(sentences))\n",
    "# s3 = ' '.join(sentences[:len_sentences])\n",
    "# s1 = ' '.join(sentences[:1])\n",
    "\n",
    "# print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time (m):  1124.06\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import io\n",
    "import time\n",
    "\n",
    "\n",
    "text = []\n",
    "title = []\n",
    "count = 0\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "Z = []\n",
    "W = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "path_cnn_downloads = './dataset/cnn/downloads'\n",
    "url =  'https://www.google.com/'\n",
    "\n",
    "filenames = glob.glob(os.path.join(path_cnn_downloads, '*.html'))\n",
    "#filenames = filenames[0:100]\n",
    "# Number of files 92579\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "article = Article(url=\"%s\" % (url), language='en')\n",
    "\n",
    "def process_article(filename):\n",
    "    article_text = \"\"\n",
    "    text_ner = \"\"\n",
    "    article_title = \"\"\n",
    "    article_keywords = \"\"\n",
    "    article_summary = \"\"\n",
    "    entities = \"\"\n",
    "    s1 = \"\"\n",
    "    s3 = \"\"\n",
    "    \n",
    "    max_ner_percent = 0.5\n",
    "    \n",
    "    try:\n",
    "        with io.open(filename, mode=\"r\", encoding=\"unicode_escape\") as f:\n",
    "            html = f.read()\n",
    "            html = html.encode('utf-8').strip()\n",
    "            #article = Article(url=\"%s\" % (url), language='en')\n",
    "            article.download(input_html=html)\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            article_text = preprocess(article.text.strip())\n",
    "            #article_title = article.title.encode('utf-8').strip()\n",
    "            article_title = preprocess(article.title.strip())\n",
    "            article_keywords = ' '.join([str(item) for item in article.keywords])\n",
    "            article_summary = preprocess(article.summary.strip())\n",
    "            entities = get_entities(article_text)\n",
    "            text_ner = get_ner_text(article_text, entities, max_ner_percent)\n",
    "            sentences = nltk.sent_tokenize(article_text)\n",
    "            len_sentences = min(3, len(sentences))\n",
    "            s3 = ' '.join(sentences[:len_sentences])\n",
    "            s1 = ' '.join(sentences[:1])\n",
    "    \n",
    "            f.close()\n",
    "    except Exception as error:\n",
    "        f.close()\n",
    "        print(\"Exception occurred: \", error)\n",
    "        return article_text, article_title, article_keywords, article_summary, text_ner, entities, s1, s3\n",
    "    return article_text, article_title, article_keywords, article_summary, text_ner, entities, s1, s3\n",
    "\n",
    "S = Parallel(n_jobs=6)(delayed(process_article)(filename) for filename in filenames)\n",
    "\n",
    "S = list(filter(None, S))\n",
    "\n",
    "print(\"Response time (m): \", round((time.time() - start)/60, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92579"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_backup = S\n",
    "text = []\n",
    "headlines = []\n",
    "keywords = []\n",
    "summary_art = []\n",
    "text_ner = []\n",
    "entities = []\n",
    "s1 = []\n",
    "s3 = []\n",
    "\n",
    "S_ = []\n",
    "\n",
    "for i in range(len(S)):\n",
    "    if len(S[i][0]) == 0 or len(S[i][1]) == 0 or len(S[i][2]) == 0 or len(S[i][3]) == 0 or len(S[i][4]) == 0 or len(S[i][5]) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        text.append(S[i][0])\n",
    "        headlines.append(S[i][1])\n",
    "        keywords.append(S[i][2])\n",
    "        summary_art.append(S[i][3])\n",
    "        text_ner.append(S[i][4])\n",
    "        entities.append(S[i][5])\n",
    "        s1.append(S[i][6])\n",
    "        s3.append(S[i][7])\n",
    "        S_.append(S[i])\n",
    "\n",
    "S = S_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Cristiano Ronaldo grabbed sixth hattrick season Real Madrid went 10 points clear archrivals Barcelona Spain 42 home win fourthplaced Levante . Sunday victory left Real driving seat win La Liga title following champions Barca 32 defeat Osasuna Saturday . Portugal star Ronaldo took league tally 27 goals season another superb display Gustavo Cabral gave Levante surprise early lead Bernabeu .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(S)):\n",
    "    if len(S[i][0]) == 0 or len(S[i][1]) == 0 or len(S[i][2]) == 0 or len(S[i][3]) ==0 or len(S[i][4]) == 0 or len(S[i][4]) == 0:\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ronaldo hattrick lifts Real Madrid 10 points clear Spain'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ronaldo hattrick lifts Real Madrid 10 points clear Spain'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'took victory league win real valencia ronaldo madrid title lifts hattrick spain second won clear points'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Cristiano Ronaldo grabbed sixth hattrick season Real Madrid went 10 points clear archrivals Barcelona Spain 42 home win fourthplaced Levante .  Sunday victory left Real driving seat win La Liga title following champions Barca 32 defeat Osasuna Saturday .  Portugal star Ronaldo took league tally 27 goals season another superb display Gustavo Cabral gave Levante surprise early lead Bernabeu .  The mathematics tell us consider league won necessary get another 39 points 13 wins told AFP .  Substitute Jonas Goncalves scored twice injury time Valencia moved within eight points Barcelona Gijon remained second bottom . '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91600"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train, S_test = train_test_split(S, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_valid, S_test = train_test_split(S_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = []\n",
    "headlines_train = []\n",
    "keywords_train = []\n",
    "summary_art_train = []\n",
    "text_ner_train = []\n",
    "entities_train = []\n",
    "s1_train = []\n",
    "s3_train = []\n",
    "\n",
    "text_valid = []\n",
    "headlines_valid = []\n",
    "keywords_valid = []\n",
    "summary_art_valid = []\n",
    "text_ner_valid = []\n",
    "entities_valid = []\n",
    "s1_valid = []\n",
    "s3_valid = []\n",
    "\n",
    "text_test = []\n",
    "headlines_test = []\n",
    "keywords_test = []\n",
    "summary_art_test = []\n",
    "text_ner_test = []\n",
    "entities_test = []\n",
    "s1_test = []\n",
    "s3_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(S_train)):  \n",
    "        text_train.append(S_train[i][0])\n",
    "        headlines_train.append(S_train[i][1])\n",
    "        keywords_train.append(S_train[i][2])\n",
    "        summary_art_train.append(S_train[i][3])\n",
    "        text_ner_train.append(S_train[i][4])\n",
    "        entities_train.append(S_train[i][5])\n",
    "        s1_train.append(S_train[i][6])\n",
    "        s3_train.append(S_train[i][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(S_valid)):  \n",
    "        text_valid.append(S_valid[i][0])\n",
    "        headlines_valid.append(S_valid[i][1])\n",
    "        keywords_valid.append(S_valid[i][2])\n",
    "        summary_art_valid.append(S_valid[i][3])\n",
    "        text_ner_valid.append(S_valid[i][4])\n",
    "        entities_valid.append(S_valid[i][5])\n",
    "        s1_valid.append(S_valid[i][6])\n",
    "        s3_valid.append(S_valid[i][7])\n",
    "        \n",
    "for i in range(len(S_test)):  \n",
    "        text_test.append(S_test[i][0])\n",
    "        headlines_test.append(S_test[i][1])\n",
    "        keywords_test.append(S_test[i][2])\n",
    "        summary_art_test.append(S_test[i][3])\n",
    "        text_ner_test.append(S_test[i][4])\n",
    "        entities_test.append(S_test[i][5])\n",
    "        s1_test.append(S_test[i][6])\n",
    "        s3_test.append(S_test[i][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73280 73280 9160 9160 9160 9160\n"
     ]
    }
   ],
   "source": [
    "print(len(text_train), len(headlines_train), len(text_valid), len(headlines_valid), len(text_test), len(headlines_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_train = {\n",
    "    'text' : text_train,\n",
    "    'headlines' : headlines_train,\n",
    "    'keywords' : keywords_train,\n",
    "    'summary_art' : summary_art_train,\n",
    "    'text_ner' : text_ner_train,\n",
    "    'entities' : entities_train,\n",
    "    's1' : s1_train,\n",
    "    's3' : s3_train\n",
    "}\n",
    "\n",
    "list_to_json_array = json.dumps(dict_train)\n",
    "\n",
    "jsonString = json.dumps(dict_train)\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_train_v3.json'), \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_valid = {\n",
    "    'text' : text_valid,\n",
    "    'headlines' : headlines_valid,\n",
    "    'keywords' : keywords_valid,\n",
    "    'summary_art' : summary_art_valid,\n",
    "    'text_ner' : text_ner_valid,\n",
    "    'entities' : entities_valid,\n",
    "    's1' : s1_valid,\n",
    "    's3' : s3_valid\n",
    "}\n",
    "\n",
    "list_to_json_array = json.dumps(dict_valid)\n",
    "\n",
    "jsonString = json.dumps(dict_valid)\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_valid_v3.json'), \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_test = {\n",
    "#     'text' : text_test,\n",
    "#     'headlines' : headlines_test,\n",
    "#     'keywords' : keywords_test,\n",
    "#     'summary_art' : summary_art_test,\n",
    "#     'text_ner' : text_ner_test,\n",
    "#     'entities' : entities_test,\n",
    "#     's1' : s1_test,\n",
    "#     's3' : s3_test\n",
    "\n",
    "# }\n",
    "\n",
    "# list_to_json_array = json.dumps(dict_test)\n",
    "\n",
    "# jsonString = json.dumps(dict_vtest)\n",
    "# jsonFile = open(os.path.join(path_processed_data, 'cnn_test_v3.json'), \"w\")\n",
    "# jsonFile.write(jsonString)\n",
    "# jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach to get sentence tokens\n",
    "# S1s = []\n",
    "# S3s = []\n",
    "# for item in X_test:\n",
    "#     sentences = re.split(r'(?<=[.:;])\\s', item)\n",
    "#     len_sentences = min(3, len(sentences))\n",
    "#     S3s.append(' '.join(sentences[:len_sentences]))\n",
    "#     S1s.append(' '.join(sentences[:1]))\n",
    "\n",
    "# print(len(S1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9160\n"
     ]
    }
   ],
   "source": [
    "size = len(text_test)\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = len(text_test)\n",
    "\n",
    "# S1s = [''] * size\n",
    "# S3s = [''] * size\n",
    "S2 = [''] * size\n",
    "S3 = [''] * size\n",
    "S4 = [''] * size\n",
    "S5 = [''] * size\n",
    "P0 = [''] * size\n",
    "P1 = [''] * size\n",
    "P2 = [''] * size\n",
    "P3 = [''] * size\n",
    "P4 = [''] * size\n",
    "P5 = [''] * size\n",
    "P6 = [''] * size\n",
    "P6 = [''] * size\n",
    "P7 = [''] * size\n",
    "\n",
    "dict_test = {\n",
    "    'text' : text_test,\n",
    "    'headlines' : headlines_test,\n",
    "    'keywords' : keywords_test,\n",
    "    'summary_art' : summary_art_test,\n",
    "    'entities' : entities_test,\n",
    "    'text_ner' : text_ner_test,\n",
    "    'sentence_1s' : s1_test,\n",
    "    'sentence_3s' : s3_test,\n",
    "    'summary_ext' : S2,\n",
    "    'summary_abs' : S3,\n",
    "    'summary_extabs' : S4,\n",
    "    'summary_ner' : S5,\n",
    "    'predict_1s' : P0,\n",
    "    'predict_3s' : P1,\n",
    "    'predict_text' : P2,\n",
    "    'predict_ext' : P3,\n",
    "    'predict_abs' : P4,\n",
    "    'predict_extabs' : P5,\n",
    "    'predict_ner' : P6,\n",
    "    'predict_t5' : P7\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(dict_test)\n",
    "\n",
    "jsonString = json.dumps(dict_test)\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_test_v3.json'), \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>headlines</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary_art</th>\n",
       "      <th>entities</th>\n",
       "      <th>text_ner</th>\n",
       "      <th>sentence_1s</th>\n",
       "      <th>sentence_3s</th>\n",
       "      <th>summary_ext</th>\n",
       "      <th>summary_abs</th>\n",
       "      <th>summary_extabs</th>\n",
       "      <th>summary_ner</th>\n",
       "      <th>predict_1s</th>\n",
       "      <th>predict_3s</th>\n",
       "      <th>predict_text</th>\n",
       "      <th>predict_ext</th>\n",
       "      <th>predict_abs</th>\n",
       "      <th>predict_extabs</th>\n",
       "      <th>predict_ner</th>\n",
       "      <th>predict_t5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story highlights Drummer Lee Riggers Rigby 25 ...</td>\n",
       "      <td>Soldier slain London street machine gunner dru...</td>\n",
       "      <td>street fusiliers member royal soldiers soldier...</td>\n",
       "      <td>Drummer Lee Rigby 25 part Regimental Recruitin...</td>\n",
       "      <td>Lee Riggers R Royal Palaces Jack London Afghan...</td>\n",
       "      <td>Story highlights Drummer Lee Riggers Rigby 25 ...</td>\n",
       "      <td>Story highlights Drummer Lee Riggers Rigby 25 ...</td>\n",
       "      <td>Story highlights Drummer Lee Riggers Rigby 25 ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York   Officials investigating reports two...</td>\n",
       "      <td>New York probes report EMTs ignored woman died</td>\n",
       "      <td>emergency department woman office report spoke...</td>\n",
       "      <td>New York   Officials investigating reports two...</td>\n",
       "      <td>New York Eutisha Renni Au Bon Pain Brooklyn Lo...</td>\n",
       "      <td>New York   Officials investigating reports two...</td>\n",
       "      <td>New York   Officials investigating reports two...</td>\n",
       "      <td>New York   Officials investigating reports two...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Editor note Share volcano photos CNN iReport ....</td>\n",
       "      <td>Erupt typical travel pattern</td>\n",
       "      <td>active island pattern erupt travel list lava t...</td>\n",
       "      <td>Photos Earth otherworldly landscapesHere list ...</td>\n",
       "      <td>CNN iReport Williams Antarctica</td>\n",
       "      <td>Editor note Share volcano photos CNN iReport ....</td>\n",
       "      <td>Editor note Share volcano photos CNN iReport .</td>\n",
       "      <td>Editor note Share volcano photos CNN iReport ....</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Ohio jury Monday begin sentencing phase c...</td>\n",
       "      <td>Sentencing phase begin convicted Ohio serial k...</td>\n",
       "      <td>ohio sowells killer women begin sowell cnn tol...</td>\n",
       "      <td>An Ohio jury Monday begin sentencing phase c...</td>\n",
       "      <td>Ohio Anthony So Cleveland Ryan Mid Cuyahoga Co...</td>\n",
       "      <td>An Ohio jury Monday begin sentencing phase c...</td>\n",
       "      <td>An Ohio jury Monday begin sentencing phase c...</td>\n",
       "      <td>An Ohio jury Monday begin sentencing phase c...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Update In July 2014 Breanna Bond 12 competed...</td>\n",
       "      <td>Girl loses 65 pounds fight childhood obesity</td>\n",
       "      <td>family childhood kids eating food parents poun...</td>\n",
       "      <td>Update In July 2014 Breanna Bond 12 competed...</td>\n",
       "      <td>Breanna Bond Olympics Heidi B Centers Disease ...</td>\n",
       "      <td>Update In July 2014 Breanna Bond 12 competed...</td>\n",
       "      <td>Update In July 2014 Breanna Bond 12 competed...</td>\n",
       "      <td>Update In July 2014 Breanna Bond 12 competed...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9155</th>\n",
       "      <td>outside experts help us establish whether thi...</td>\n",
       "      <td>What evidence Syrian chemical weapons attack</td>\n",
       "      <td>intelligence chemical syrian whats evidence us...</td>\n",
       "      <td>And Syrian President Bashar alAssad regime say...</td>\n",
       "      <td>Day Damascus France Germany BND Gerhard Schind...</td>\n",
       "      <td>outside experts help us establish whether thi...</td>\n",
       "      <td>outside experts help us establish whether thi...</td>\n",
       "      <td>outside experts help us establish whether thi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156</th>\n",
       "      <td>West Texas   Since little boys growing West Te...</td>\n",
       "      <td>A town copes lives lost Texas blast</td>\n",
       "      <td>lives west school texas community students off...</td>\n",
       "      <td>West Texas   Since little boys growing West Te...</td>\n",
       "      <td>West Texas Doug Robert S Fertilizer Co Kenneth...</td>\n",
       "      <td>West Texas   Since little boys growing West Te...</td>\n",
       "      <td>West Texas   Since little boys growing West Te...</td>\n",
       "      <td>West Texas   Since little boys growing West Te...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9157</th>\n",
       "      <td>Government Accountability Office released rep...</td>\n",
       "      <td>Voluntary recall another compounding facility</td>\n",
       "      <td>voluntary facility compounding fda levels pati...</td>\n",
       "      <td>The FDA issued national voluntary sterileuse p...</td>\n",
       "      <td>Government Accountability Office FDA U . S</td>\n",
       "      <td>Government Accountability Office released rep...</td>\n",
       "      <td>Government Accountability Office released rep...</td>\n",
       "      <td>Government Accountability Office released rep...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9158</th>\n",
       "      <td>Editor note Amy Gahran writes mobile tech CNN ...</td>\n",
       "      <td>Crowdsourced maps help mobile users compare ne...</td>\n",
       "      <td>wireless network maps data reliability strengt...</td>\n",
       "      <td>As about application runs background monitorin...</td>\n",
       "      <td>Amy Gahran CNN . com San Francisco Bay Area Co...</td>\n",
       "      <td>Editor note Amy Gahran writes mobile tech CNN ...</td>\n",
       "      <td>Editor note Amy Gahran writes mobile tech CNN .</td>\n",
       "      <td>Editor note Amy Gahran writes mobile tech CNN ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>Story highlights Victims bus bomb hospital att...</td>\n",
       "      <td>Survivors shocked double attack</td>\n",
       "      <td>women young shocked hospital bombing pakistan ...</td>\n",
       "      <td>All remains bus students boarded travel home d...</td>\n",
       "      <td>Pakistan Frontier Corps VI Rep Yasmin Baloc</td>\n",
       "      <td>Story highlights Victims bus bomb hospital att...</td>\n",
       "      <td>Story highlights Victims bus bomb hospital att...</td>\n",
       "      <td>Story highlights Victims bus bomb hospital att...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9160 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Story highlights Drummer Lee Riggers Rigby 25 ...   \n",
       "1     New York   Officials investigating reports two...   \n",
       "2     Editor note Share volcano photos CNN iReport ....   \n",
       "3       An Ohio jury Monday begin sentencing phase c...   \n",
       "4       Update In July 2014 Breanna Bond 12 competed...   \n",
       "...                                                 ...   \n",
       "9155   outside experts help us establish whether thi...   \n",
       "9156  West Texas   Since little boys growing West Te...   \n",
       "9157   Government Accountability Office released rep...   \n",
       "9158  Editor note Amy Gahran writes mobile tech CNN ...   \n",
       "9159  Story highlights Victims bus bomb hospital att...   \n",
       "\n",
       "                                              headlines  \\\n",
       "0     Soldier slain London street machine gunner dru...   \n",
       "1        New York probes report EMTs ignored woman died   \n",
       "2                          Erupt typical travel pattern   \n",
       "3     Sentencing phase begin convicted Ohio serial k...   \n",
       "4          Girl loses 65 pounds fight childhood obesity   \n",
       "...                                                 ...   \n",
       "9155       What evidence Syrian chemical weapons attack   \n",
       "9156                A town copes lives lost Texas blast   \n",
       "9157      Voluntary recall another compounding facility   \n",
       "9158  Crowdsourced maps help mobile users compare ne...   \n",
       "9159                    Survivors shocked double attack   \n",
       "\n",
       "                                               keywords  \\\n",
       "0     street fusiliers member royal soldiers soldier...   \n",
       "1     emergency department woman office report spoke...   \n",
       "2     active island pattern erupt travel list lava t...   \n",
       "3     ohio sowells killer women begin sowell cnn tol...   \n",
       "4     family childhood kids eating food parents poun...   \n",
       "...                                                 ...   \n",
       "9155  intelligence chemical syrian whats evidence us...   \n",
       "9156  lives west school texas community students off...   \n",
       "9157  voluntary facility compounding fda levels pati...   \n",
       "9158  wireless network maps data reliability strengt...   \n",
       "9159  women young shocked hospital bombing pakistan ...   \n",
       "\n",
       "                                            summary_art  \\\n",
       "0     Drummer Lee Rigby 25 part Regimental Recruitin...   \n",
       "1     New York   Officials investigating reports two...   \n",
       "2     Photos Earth otherworldly landscapesHere list ...   \n",
       "3       An Ohio jury Monday begin sentencing phase c...   \n",
       "4       Update In July 2014 Breanna Bond 12 competed...   \n",
       "...                                                 ...   \n",
       "9155  And Syrian President Bashar alAssad regime say...   \n",
       "9156  West Texas   Since little boys growing West Te...   \n",
       "9157  The FDA issued national voluntary sterileuse p...   \n",
       "9158  As about application runs background monitorin...   \n",
       "9159  All remains bus students boarded travel home d...   \n",
       "\n",
       "                                               entities  \\\n",
       "0     Lee Riggers R Royal Palaces Jack London Afghan...   \n",
       "1     New York Eutisha Renni Au Bon Pain Brooklyn Lo...   \n",
       "2                       CNN iReport Williams Antarctica   \n",
       "3     Ohio Anthony So Cleveland Ryan Mid Cuyahoga Co...   \n",
       "4     Breanna Bond Olympics Heidi B Centers Disease ...   \n",
       "...                                                 ...   \n",
       "9155  Day Damascus France Germany BND Gerhard Schind...   \n",
       "9156  West Texas Doug Robert S Fertilizer Co Kenneth...   \n",
       "9157         Government Accountability Office FDA U . S   \n",
       "9158  Amy Gahran CNN . com San Francisco Bay Area Co...   \n",
       "9159        Pakistan Frontier Corps VI Rep Yasmin Baloc   \n",
       "\n",
       "                                               text_ner  \\\n",
       "0     Story highlights Drummer Lee Riggers Rigby 25 ...   \n",
       "1     New York   Officials investigating reports two...   \n",
       "2     Editor note Share volcano photos CNN iReport ....   \n",
       "3       An Ohio jury Monday begin sentencing phase c...   \n",
       "4       Update In July 2014 Breanna Bond 12 competed...   \n",
       "...                                                 ...   \n",
       "9155   outside experts help us establish whether thi...   \n",
       "9156  West Texas   Since little boys growing West Te...   \n",
       "9157   Government Accountability Office released rep...   \n",
       "9158  Editor note Amy Gahran writes mobile tech CNN ...   \n",
       "9159  Story highlights Victims bus bomb hospital att...   \n",
       "\n",
       "                                            sentence_1s  \\\n",
       "0     Story highlights Drummer Lee Riggers Rigby 25 ...   \n",
       "1     New York   Officials investigating reports two...   \n",
       "2        Editor note Share volcano photos CNN iReport .   \n",
       "3       An Ohio jury Monday begin sentencing phase c...   \n",
       "4       Update In July 2014 Breanna Bond 12 competed...   \n",
       "...                                                 ...   \n",
       "9155   outside experts help us establish whether thi...   \n",
       "9156  West Texas   Since little boys growing West Te...   \n",
       "9157   Government Accountability Office released rep...   \n",
       "9158    Editor note Amy Gahran writes mobile tech CNN .   \n",
       "9159  Story highlights Victims bus bomb hospital att...   \n",
       "\n",
       "                                            sentence_3s summary_ext  \\\n",
       "0     Story highlights Drummer Lee Riggers Rigby 25 ...               \n",
       "1     New York   Officials investigating reports two...               \n",
       "2     Editor note Share volcano photos CNN iReport ....               \n",
       "3       An Ohio jury Monday begin sentencing phase c...               \n",
       "4       Update In July 2014 Breanna Bond 12 competed...               \n",
       "...                                                 ...         ...   \n",
       "9155   outside experts help us establish whether thi...               \n",
       "9156  West Texas   Since little boys growing West Te...               \n",
       "9157   Government Accountability Office released rep...               \n",
       "9158  Editor note Amy Gahran writes mobile tech CNN ...               \n",
       "9159  Story highlights Victims bus bomb hospital att...               \n",
       "\n",
       "     summary_abs summary_extabs summary_ner predict_1s predict_3s  \\\n",
       "0                                                                   \n",
       "1                                                                   \n",
       "2                                                                   \n",
       "3                                                                   \n",
       "4                                                                   \n",
       "...          ...            ...         ...        ...        ...   \n",
       "9155                                                                \n",
       "9156                                                                \n",
       "9157                                                                \n",
       "9158                                                                \n",
       "9159                                                                \n",
       "\n",
       "     predict_text predict_ext predict_abs predict_extabs predict_ner  \\\n",
       "0                                                                      \n",
       "1                                                                      \n",
       "2                                                                      \n",
       "3                                                                      \n",
       "4                                                                      \n",
       "...           ...         ...         ...            ...         ...   \n",
       "9155                                                                   \n",
       "9156                                                                   \n",
       "9157                                                                   \n",
       "9158                                                                   \n",
       "9159                                                                   \n",
       "\n",
       "     predict_t5  \n",
       "0                \n",
       "1                \n",
       "2                \n",
       "3                \n",
       "4                \n",
       "...         ...  \n",
       "9155             \n",
       "9156             \n",
       "9157             \n",
       "9158             \n",
       "9159             \n",
       "\n",
       "[9160 rows x 20 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_train_v3.json'), \"r\")\n",
    "jsonContent = jsonFile.read()\n",
    "details_dict = json.loads(jsonContent)\n",
    "jsonFile.close()\n",
    "\n",
    "text_train = details_dict['text']\n",
    "headlines_train = details_dict['headlines']\n",
    "keywords_train = details_dict['keywords']\n",
    "\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_valid_v3.json'), \"r\")\n",
    "jsonContent = jsonFile.read()\n",
    "details_dict = json.loads(jsonContent)\n",
    "jsonFile.close()\n",
    "\n",
    "text_valid = details_dict['text']\n",
    "headlines_valid = details_dict['headlines']\n",
    "keywords_valid = details_dict['keywords']\n",
    "\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_test_v3.json'), \"r\")\n",
    "jsonContent = jsonFile.read()\n",
    "details_dict = json.loads(jsonContent)\n",
    "jsonFile.close()\n",
    "\n",
    "X_test = details_dict['text']\n",
    "headlines_test = details_dict['headlines']\n",
    "keywords_test = details_dict['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Story highlights Drummer Lee Riggers Rigby 25 machine gunner became recruiter He real character infantry commanding officer says As drummer stood outside Royal Palaces He also father 2yearold son Jack The British soldier slain gruesome cleaver attack London wellliked infantryman machine gunner served Afghanistan Cyprus became military recruiter ceremonial drummer outside royal palaces military said Thursday .  Drummer Lee Rigby 25 part Regimental Recruiting Team London machine gunner part 2nd Battalion Royal Regiment Fusiliers .  The Fusiliers infantry group known hackle feather plume military headdress .  Rigby 2yearold son Jack UK Ministry Defense said .  Witness Attackers were animals Fellow soldiers described engaging personality .  He joined army 2006 acquired nickname Riggers platoon .  He real character within Second Fusiliers Lt .  Col .  Jim Taylor commanding officer 2nd Fusiliers said statement .  Larger life heart Corps Drums .  An experienced talented side drummer machine gunner true warrior served distinction Afghanistan Germany Cyprus .  Rigby also loved soccer Manchester United soldiers said .  He one Battalion great characters always smiling always ready brighten mood fellow Fusiliers Warrant Officer Ned Miller 2nd Fusiliers said statement .  He easily identified whilst parade huge smile face proud member Drums .  He would always stop chat tell Manchester United would win league again .  Rigby born Crumpsall Manchester .  After joining army first post Cyprus machine gunner Dhekelia military said .  In 2008 assigned Hounslow West London became an integral member Corps Drums throughout Battalion time public duties highlight part Household Division Beating Retreat  real honour line infantry Corps Drums ministry said .  London attack mirrors plot behead Muslim soldier In 2009 deployed operations for first time sent Afghanistan Helmand province member fire support group Patrol Base Woqab .  He returned UK completed second tour public duties .  Later followed battalion Celle Germany .  In 2011 became recruiter Regimental Headquarters Tower London .  London attack Terrorist targeting soldiers home again Rigby was cheeky humorous man always joke brighten mood extremely popular member Fire Support Group said Capt .  Alan Williamson Rigby platoon commander 2010 2011 .  Rigby death attracted worldwide attention slaying scene captured cell phone camera . ']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Soldier slain London street machine gunner drummer dad']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street fusiliers member royal soldiers soldier slain gunner drummer machine military dad real london drums']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_test[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "v3.m_w266_FP_NB2_Text_Summarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f5f3c54cfe94744a5edc2249f9980ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2359eb5cbae14f9491a0773266689b88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c0fffc55cf36497597dd17d5deab16ed",
       "IPY_MODEL_970afe25185c4ab5bb46d9bcc7cdef05",
       "IPY_MODEL_7a3a9ac7179d44989553ed77db5a1d30"
      ],
      "layout": "IPY_MODEL_61bb9a947d354113a5aea94614bbc16d"
     }
    },
    "43bd954abc734213847d1b133e35045e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d66f67729914763afd6ce4c76457169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61bb9a947d354113a5aea94614bbc16d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d75bd691e6348fbae23f4a3cec3d3f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a3a9ac7179d44989553ed77db5a1d30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43bd954abc734213847d1b133e35045e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5d66f67729914763afd6ce4c76457169",
      "value": " 0/158331 [00:01&lt;?, ?it/s]"
     }
    },
    "970afe25185c4ab5bb46d9bcc7cdef05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c966b37ee8cc441e8a47248aec1b62bb",
      "max": 158331,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d75bd691e6348fbae23f4a3cec3d3f1",
      "value": 0
     }
    },
    "9ec381d401c746a4bbe20fc08be64520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0fffc55cf36497597dd17d5deab16ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ec381d401c746a4bbe20fc08be64520",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1f5f3c54cfe94744a5edc2249f9980ff",
      "value": "  0%"
     }
    },
    "c966b37ee8cc441e8a47248aec1b62bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
