{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUPGDF4eEVyL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "\n",
    "# Better pandas table\n",
    "# from google.colab import data_table\n",
    "# data_table.enable_dataframe_formatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7PM7iZmvBaN",
    "outputId": "fb8b16a6-dde3-41fc-cf1e-ea9ed1fb8b6e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install bert-extractive-summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g88emQUOwJLE",
    "outputId": "fbc9c611-b1fa-4604-d42d-9f2aea0a2afd"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qk6OnnphsEeZ"
   },
   "outputs": [],
   "source": [
    "# path_models = '/content/drive/MyDrive/w266/models/'\n",
    "# path_outputs = '/content/drive/MyDrive/w266/outputs/'\n",
    "# path_processed_data= '/content/drive/MyDrive/w266/processed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = './models/'\n",
    "path_outputs = './outputs/'\n",
    "path_processed_data= './processed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGrynIHx7NBk"
   },
   "outputs": [],
   "source": [
    "data_name = 'cnn_test'\n",
    "input_filename = data_name + '_v3.json' \n",
    "#input_filename = data_name + '_summary_v2.json' \n",
    "#out_filename = data_name + '_summary_abs.json'\n",
    "out_filename = data_name + '_summary_v3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = open(os.path.join(path_processed_data, input_filename), \"r\")\n",
    "jsonContent = jsonFile.read()\n",
    "details_dict = json.loads(jsonContent)\n",
    "jsonFile.close()\n",
    "\n",
    "\n",
    "X = details_dict['text']\n",
    "size = len(X)\n",
    "\n",
    "Y = details_dict['headlines']\n",
    "Z = details_dict['keywords']\n",
    "E = details_dict['entities']\n",
    "text_ner = details_dict['text_ner']\n",
    "S1s = details_dict['sentence_1s']\n",
    "S3s = details_dict['sentence_3s']\n",
    "S1 = details_dict['summary_art']\n",
    "S2 = details_dict['summary_ext']\n",
    "S3 = details_dict['summary_abs']\n",
    "S4 = details_dict['summary_extabs']\n",
    "S5 = details_dict['summary_ner']\n",
    "#S6 = details_dict['summary_t5']\n",
    "S6 = ['']*size\n",
    "\n",
    "P0 = details_dict['predict_1s']\n",
    "P1 = details_dict['predict_3s']\n",
    "P2 = details_dict['predict_text']\n",
    "P3 = details_dict['predict_ext']\n",
    "P4 = details_dict['predict_abs']\n",
    "P5 = details_dict['predict_extabs']\n",
    "P6 = details_dict['predict_ner']\n",
    "P7 = details_dict['predict_t5']\n",
    "\n",
    "\n",
    "details = {\n",
    "    \n",
    "    'text' : X,\n",
    "    'headlines' : Y,\n",
    "    'keywords' : Z,\n",
    "    'summary_art' : S1,\n",
    "    'entities' : E,\n",
    "    'text_ner' : text_ner,\n",
    "    'sentence_1s' : S1s,\n",
    "    'sentence_3s' : S3s,\n",
    "    'summary_ext' : S2,\n",
    "    'summary_abs' : S3,\n",
    "    'summary_extabs' : S4,\n",
    "    'summary_ner' : S5,\n",
    "    'summary_t5' : S6,\n",
    "    'predict_1s' : P0,\n",
    "    'predict_3s' : P1,\n",
    "    'predict_text' : P2,\n",
    "    'predict_ext' : P3,\n",
    "    'predict_abs' : P4,\n",
    "    'predict_extabs' : P5,\n",
    "    'predict_ner' : P6,\n",
    "    'predict_t5' : P7\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# size = len(X)\n",
    "\n",
    "# S2 = [\"\"] * size\n",
    "# S3 = [\"\"] * size\n",
    "# S4 = [\"\"] * size\n",
    "# S5 = [\"\"] * size\n",
    "# P1 = [\"\"] * size\n",
    "# P2 = [\"\"] * size\n",
    "# P3 = [\"\"] * size\n",
    "# P4 = [\"\"] * size\n",
    "# P5 = [\"\"] * size\n",
    "\n",
    "# details = {\n",
    "#     'text' : X,\n",
    "#     'headlines' : Y,\n",
    "#     'keywords' : Z,\n",
    "#     'summary_art' : S1,\n",
    "#     'summary_ext' : S2,\n",
    "#     'summary_abs' : S3,\n",
    "#     'summary_extabs' : S4,\n",
    "#     'summary_ner' : S5,\n",
    "#     'predict_text' : P1,\n",
    "#     'predict_ext' : P2,\n",
    "#     'predict_abs' : P3,\n",
    "#     'predict_extabs' : P4,\n",
    "#     'predict_ner' : P5\n",
    "# }\n",
    "\n",
    "df_test = pd.DataFrame(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QjvSHFWvBaQ",
    "outputId": "b567b20e-8a70-4c64-bf35-9545b22db85d"
   },
   "outputs": [],
   "source": [
    "len(P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BRB5kMc3Wyr"
   },
   "outputs": [],
   "source": [
    "def current_time_min():\n",
    "    time_in_mins = time.time()/60.0\n",
    "    return round(time_in_mins, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer.sbert import SBertSummarizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "#abs_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "abs_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "ext_model = SBertSummarizer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_ners(article, entities, max_sentence_percent):\n",
    "\n",
    "    #print(entities)\n",
    "    entities_array = entities.split()\n",
    "    # returns a string with ner sentences\n",
    "\n",
    "    sentences_array = []\n",
    "    ner_sentences = []\n",
    "\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    #print(sentences)\n",
    "    sentence_dict = {}\n",
    "    sentence_order_dict = {}\n",
    "    for k, sentence in enumerate(sentences):\n",
    "        sentence_order_dict[sentence] = k\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    #print(\"sentence count:\", sentence_count)\n",
    "    \n",
    "    max_sentence_count = int(sentence_count * max_sentence_percent)\n",
    "    #for sentence in sentences:\n",
    "    for k, sentence in enumerate(sentences): \n",
    "        count = 0\n",
    "        for word in entities_array:\n",
    "            if word in sentence:\n",
    "                count += 1\n",
    "        #print(count)\n",
    "        sentence_dict[sentence] = count\n",
    "    \n",
    "    sorted_sd = sorted(sentence_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    for item in sorted_sd:\n",
    "        ner_sentences.append(''.join(item[0]))\n",
    "        #print(item[0], item[1])\n",
    "    \n",
    "    sentence_dict = {}\n",
    "                \n",
    "    for sentence in ner_sentences[:max_sentence_count]:\n",
    "        sentence_dict[sentence] = sentence_order_dict[sentence]\n",
    "        \n",
    "    sorted_sd = sorted(sentence_dict.items(), key=operator.itemgetter(1), reverse=False)\n",
    "\n",
    "    ner_sentences = []\n",
    "    for item in sorted_sd:\n",
    "        ner_sentences.append(''.join(item[0]))\n",
    "\n",
    "    ner_sentences = ' '.join(ner_sentences)\n",
    "            \n",
    "    if len(ner_sentences) == 0:\n",
    "        for item in sentences:\n",
    "            len_sentences = min(max_sentence_count, len(sentences))\n",
    "            ner_sentences = ' '.join(sentences[:len_sentences])\n",
    "    \n",
    "    return ner_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = \"On New Year Day pioneering pilots celebrated float 125th Tournament Roses parade Pasadena. California part effort bring attention often overlooked part American history. Women pilots honored 65 years World War II service First lady Eleanor Roosevelt early supporter program writing September 1942 newspaper column This time women patient .  We war need fight ability every weapon possible .  Women pilots particular case weapon waiting used. Mary Matalin: The book good read unusually detailed frontrow seat view strained campaigns always are compelling insight Palin perspective. But impact personal professional relationships sad one indeed one. I hope conservatives let divide us marching toward promising midterm reflects ascendant commonsense conservatism requires good guys foxhole together. Cameron Todd Willingham executed 2004 13 years fire killed three daughters. Prosecutors argued Willingham deliberately set 1991 blaze  three reviews evidence outside experts found fire ruled arson. Texas Forensic Science Commission looking Willingham execution since 2008.\"\n",
    "# entities = \"Pasadena American Eleanor Roosevelt Willingham Palin Mary\"\n",
    "# summary_ner = summary_ners(article, entities, 0.3)\n",
    "# #summary_ner\n",
    "# #sentences = nltk.sent_tokenize(summary_ner)\n",
    "# #print(len (sentences))\n",
    "# print(summary_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_ners_original(article, entities, max_sentence_count):\n",
    "\n",
    "    # max_sentence_count = 50% & order retained\n",
    "    \n",
    "    print(entities)\n",
    "    entities_array = entities.split()\n",
    "    # returns a string with ner sentences\n",
    "\n",
    "    sentences_array = []\n",
    "    ner_sentences = []\n",
    "\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    #print(sentences)\n",
    "    sentence_dict = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        count = 0\n",
    "        for word in entities_array:\n",
    "            if word in sentence:\n",
    "                count += 1\n",
    "        #print(count)\n",
    "        sentence_dict[sentence] = count\n",
    "    \n",
    "    sorted_sd = sorted(sentence_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    for item in sorted_sd:\n",
    "        ner_sentences.append(''.join(item[0]))\n",
    "\n",
    "    ner_sentences = ' '.join(ner_sentences[:max_sentence_count])\n",
    "    \n",
    "    if len(ner_sentences) == 0:\n",
    "        for item in sentences:\n",
    "            len_sentences = min(max_sentence_count, len(sentences))\n",
    "            ner_sentences = ' '.join(sentences[:len_sentences])\n",
    "    \n",
    "    return ner_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://pypi.org/project/bert-extractive-summarizer/\n",
    "# # https://arxiv.org/abs/1908.10084\n",
    "\n",
    "df = df_test\n",
    "\n",
    "#abs_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "#abs_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "ext_model = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def summarize_text(row):\n",
    "    \n",
    "    text = row[1]['text']\n",
    "    headlines = row[1]['headlines']\n",
    "    keywords = row[1]['keywords']\n",
    "    entities = row[1]['entities']\n",
    "    sentence_1s = row[1]['sentence_1s']\n",
    "    sentence_3s = row[1]['sentence_3s']\n",
    "   \n",
    "    summary_art = row[1]['summary_art']\n",
    "    summary_ext = row[1]['summary_ext']\n",
    "    summary_abs = row[1]['summary_abs']\n",
    "    summary_extabs = row[1]['summary_extabs']\n",
    "    summary_ner = row[1]['summary_ner']\n",
    "    summary_t5 = row[1]['summary_t5']\n",
    "    max_len = min(len(summary_abs), 280)\n",
    "    max_sentence_percent = 0.5\n",
    "    \n",
    "    try:\n",
    "        #ext_model = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
    "        result = ext_model(text, num_sentences=10)\n",
    "        summary_ext = ''.join(result)\n",
    "        summary_extabs = abs_summarizer(summary_ext, max_length=min(len(summary_ext), 280), min_length=15, do_sample=False)[0]['summary_text']\n",
    "        #summary_ner = summary_ners(text, entities, max_sentence_percent)\n",
    "        summary_abs = abs_summarizer(text, max_length=min(len(text), 280), min_length=15, do_sample=False)[0]['summary_text']\n",
    "    except Exception as error:\n",
    "        summary_abs = abs_summarizer(text[:2048], max_length=min(len(text), 280), min_length=15, do_sample=False)[0]['summary_text']\n",
    "        return text, headlines, keywords, entities, sentence_1s, sentence_3s, summary_art, summary_ext, summary_abs,  summary_extabs, summary_ner, summary_t5\n",
    "    return text, headlines, keywords, entities, sentence_1s, sentence_3s, summary_art, summary_ext, summary_abs,  summary_extabs, summary_ner, summary_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = []\n",
    "headlines = []\n",
    "keywords = []\n",
    "entities = []\n",
    "sentence_1s = []\n",
    "sentence_3s = []\n",
    "summary_art = []\n",
    "summary_ext = []\n",
    "summary_abs = []\n",
    "summary_extabs = []\n",
    "summary_ner = []\n",
    "summary_t5 = []\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "start = time.time()\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "S = Parallel(n_jobs=4)(delayed(summarize_text)(row) for row in df.iterrows())\n",
    "\n",
    "print(\"Response time (mins): \", round((time.time() - start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = list(filter(None, S))\n",
    "for i in range(len(S)):  \n",
    "        text.append(S[i][0])  # text\n",
    "        headlines.append(S[i][1])  # headlines\n",
    "        keywords.append(S[i][2])  # keywords\n",
    "        entities.append(S[i][3])  # entities\n",
    "        sentence_1s.append(S[i][4])  # sentence_1s\n",
    "        sentence_3s.append(S[i][5])  # sentence_3s\n",
    "        summary_art.append(S[i][6])  # summary_art\n",
    "        summary_ext.append(S[i][7])  # summary_ext\n",
    "        summary_abs.append(S[i][8])  # summary_abs\n",
    "        summary_extabs.append(S[i][9])  # summary_extabs\n",
    "        summary_ner.append(S[i][10])  # summary_ner\n",
    "        summary_t5.append(S[i][11])  # summary_t5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_ner[0]\n",
    "# sentences = nltk.sent_tokenize(summary_ner[0])\n",
    "# print(len (sentences))\n",
    "# print(summary_ner[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"On New Year Day pioneering pilots celebrated float 125th Tournament Roses parade Pasadena California part effort bring attention often overlooked part American history. Women pilots honored 65 years World War II service First lady Eleanor Roosevelt early supporter program writing September 1942 newspaper column This time women patient .  We war need fight ability every weapon possible .  Women pilots particular case weapon waiting used., On New Year Day pioneering pilots celebrated float 125th Tournament Roses parade Pasadena California part effort bring attention often overlooked part American history. Women pilots honored 65 years World War II service First lady Eleanor Roosevelt early supporter program writing September 1942 newspaper column This time women patient .  We war need fight ability every weapon possible .  Women pilots particular case weapon waiting used., On New Year Day pioneering pilots celebrated float 125th Tournament Roses parade Pasadena California part effort bring attention often overlooked part American history. Women pilots honored 65 years World War II service First lady Eleanor Roosevelt early supporter program writing September 1942 newspaper column This time women patient .  We war need fight ability every weapon possible .  Women pilots particular case weapon waiting used. On New Year Day pioneering pilots celebrated float 125th Tournament Roses parade Pasadena California part effort bring attention often overlooked part American history. Women pilots honored 65 years World War II service First lady Eleanor Roosevelt early supporter program writing September 1942 newspaper column This time women patient .  We war need fight ability every weapon possible .  Women pilots particular case weapon waiting used. On New Year Day pioneering pilots celebrated float 125th Tournament Roses parade Pasadena California part effort bring attention often overlooked part American history. Women pilots honored 65 years World War II service First lady Eleanor Roosevelt early supporter program writing September 1942 newspaper column This time women patient .  We war need fight ability every weapon possible .  Women pilots particular case weapon waiting used.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "# t5_prepared_Text = \"summarize: \" + preprocess_text\n",
    "# print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "# tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# # summmarize \n",
    "# summary_ids = model.generate(tokenized_text,\n",
    "#                                     num_beams=4,\n",
    "#                                     no_repeat_ngram_size=2,\n",
    "#                                     min_length=30,\n",
    "#                                     max_length=100,\n",
    "#                                     early_stopping=True)\n",
    "\n",
    "# output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df_test\n",
    "\n",
    "def summarize_text_t5(row):\n",
    "    \n",
    "    text = row[1]['text']\n",
    "    #.split(' ', 512)\n",
    "    #text = ' '.join([str(item) for item in text])\n",
    "    max_len = min(len(text), 280)\n",
    "    max_sentence_count = 5\n",
    "    summary_t5 = []\n",
    "    \n",
    "    try:\n",
    "        t5_prepared_Text = \"summarize: \" + text\n",
    "        tokenized_text = tokenizer.encode(t5_prepared_Text, max_length=2048, return_tensors=\"pt\").to(device) #default 512\n",
    "        summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=max_len,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "        summary_t5 = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "    except Exception as error:\n",
    "        t5_prepared_Text = \"summarize: \" + text\n",
    "        tokenized_text = tokenizer.encode(t5_prepared_Text, max_length=6192, return_tensors=\"pt\").to(device) #default 512\n",
    "        summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=max_len,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "        summary_t5 = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return text, summary_t5\n",
    "    return text, summary_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = []\n",
    "# headlines = []\n",
    "# keywords = []\n",
    "# entities = []\n",
    "# sentence_1s = []\n",
    "# sentence_3s = []\n",
    "# summary_art = []\n",
    "# summary_ext = []\n",
    "# summary_abs = []\n",
    "# summary_extabs = []\n",
    "# summary_ner = []\n",
    "summary_t5 = []\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "start = time.time()\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "S = Parallel(n_jobs=4)(delayed(summarize_text_t5)(row) for row in df.iterrows())\n",
    "\n",
    "print(\"Response time (mins): \", round((time.time() - start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_t5 = []\n",
    "text = []\n",
    "S = list(filter(None, S))\n",
    "for i in range(len(S)):  \n",
    "        text.append(S[i][0])  # text\n",
    "        summary_t5.append(S[i][1])  # summary_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = details_dict['text']\n",
    "# Y = details_dict['headlines']\n",
    "# Z = details_dict['keywords']\n",
    "# S1 = details_dict['summary_art']\n",
    "# S2 = details_dict['summary_ext']\n",
    "# S3 = details_dict['summary_abs']\n",
    "# S4 = details_dict['summary_extabs']\n",
    "# S5 = details_dict['summary_ner']\n",
    "# P1 = details_dict['predict_base']\n",
    "# P2 = details_dict['predict_ext']\n",
    "# P3 = details_dict['predict_abs']\n",
    "# P4 = details_dict['predict_extabs']\n",
    "# P5 = details_dict['predict_ner']\n",
    "\n",
    "\n",
    "#X = details_dict['text']\n",
    "#Y = details_dict['headlines']\n",
    "#Z = details_dict['keywords']\n",
    "# S1 = W\n",
    "# S2 = V\n",
    "# S3 = U\n",
    "# S4 = [''] * size\n",
    "#S5 = [''] * size\n",
    "\n",
    "P0 = [''] * size\n",
    "P1 = [''] * size\n",
    "P2 = [''] * size\n",
    "P3 = [''] * size\n",
    "P4 = [''] * size\n",
    "P5 = [''] * size\n",
    "P6 = [''] * size\n",
    "P7 = [''] * size\n",
    "\n",
    "details = {\n",
    "    'headlines' : headlines,\n",
    "    'text' : text,\n",
    "    'keywords' : keywords,\n",
    "    'summary_art' : summary_art,\n",
    "    'entities' : entities,\n",
    "    'text_ner' : text_ner,\n",
    "    'sentence_1s' : sentence_1s,\n",
    "    'sentence_3s' : sentence_3s,\n",
    "    'summary_ext' : summary_ext,\n",
    "    'summary_abs' : summary_abs,\n",
    "    'summary_extabs' : summary_extabs,\n",
    "    'summary_ner' : summary_ner,\n",
    "    'summary_t5' : summary_t5,\n",
    "    'predict_1s' : P0,\n",
    "    'predict_3s' : P1,\n",
    "    'predict_text' : P2,\n",
    "    'predict_ext' : P3,\n",
    "    'predict_abs' : P4,\n",
    "    'predict_extabs' : P5,\n",
    "    'predict_ner' : P6,\n",
    "    'predict_t5' : P7\n",
    "}\n",
    "\n",
    "\n",
    "# details = {\n",
    "#     'headlines' : Y,\n",
    "#     'text' : X,\n",
    "#     'keywords' : Z,\n",
    "#     'summary_art' : S1,\n",
    "#     'entities' : E,\n",
    "#     'sentence_1s' : S1s,\n",
    "#     'sentence_3s' : S3s,\n",
    "#     'summary_ext' : S2,\n",
    "#     'summary_abs' : S3,\n",
    "#     'summary_extabs' : S4,\n",
    "#     'summary_ner' : S5,\n",
    "#     'summary_t5' : summary_t5,\n",
    "#     'predict_1s' : P0,\n",
    "#     'predict_3s' : P1,\n",
    "#     'predict_text' : P2,\n",
    "#     'predict_ext' : P3,\n",
    "#     'predict_abs' : P4,\n",
    "#     'predict_extabs' : P5,\n",
    "#     'predict_ner' : P6,\n",
    "#     'predict_t5' : P7\n",
    "# }\n",
    "\n",
    "\n",
    "# details = {\n",
    "#     'text' : text,\n",
    "#     'headlines' : headlines,\n",
    "#     'keywords' : keywords,\n",
    "#     'summary_art' : summary_art,\n",
    "#     'summary_ext' : summary_ext,\n",
    "#     'summary_abs' : summary_abs,\n",
    "#     'summary_extabs' : summary_extabs,\n",
    "#     'summary_ner' : S5,\n",
    "#     'predict_base' : P1,\n",
    "#     'predict_ext' : P2,\n",
    "#     'predict_abs' : P3,\n",
    "#     'predict_extabs' : P4,\n",
    "#     'predict_ner' : P5\n",
    "# }\n",
    "\n",
    "#df_test = pd.DataFrame(details)\n",
    "\n",
    "jsonString = json.dumps(details)\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_test_summary_v3.json'), \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = {k: details[k] for k in ('headlines', 'summary_art', 'summary_ext', 'summary_abs')}\n",
    "df_results = {k: details[k] for k in ('summary_ext', 'summary_abs', 'summary_extabs', 'summary_ner', 'summary_t5')}\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(HTML(df_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = {k: details[k] for k in ('headlines', 'summary_art', 'summary_ext', 'summary_abs')}\n",
    "df_results = {k: details[k] for k in ('sentence_1s', 'sentence_3s', 'text')}\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(HTML(df_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = open(os.path.join(path_processed_data, input_filename), \"r\")\n",
    "jsonContent = jsonFile.read()\n",
    "details_dict = json.loads(jsonContent)\n",
    "jsonFile.close()\n",
    "\n",
    "\n",
    "X = details_dict['text']\n",
    "size = len(X)\n",
    "\n",
    "Y = details_dict['headlines']\n",
    "Z = details_dict['keywords']\n",
    "E = details_dict['entities']\n",
    "text_ner = details_dict['text_ner']\n",
    "S1s = details_dict['sentence_1s']\n",
    "S3s = details_dict['sentence_3s']\n",
    "S1 = details_dict['summary_art']\n",
    "S2 = details_dict['summary_ext']\n",
    "S3 = details_dict['summary_abs']\n",
    "S4 = details_dict['summary_extabs']\n",
    "S5 = details_dict['summary_ner']\n",
    "#S6 = details_dict['summary_t5']\n",
    "S6 = ['']*size\n",
    "\n",
    "P0 = details_dict['predict_1s']\n",
    "P1 = details_dict['predict_3s']\n",
    "P2 = details_dict['predict_text']\n",
    "P3 = details_dict['predict_ext']\n",
    "P4 = details_dict['predict_abs']\n",
    "P5 = details_dict['predict_extabs']\n",
    "P6 = details_dict['predict_ner']\n",
    "P7 = details_dict['predict_t5']\n",
    "\n",
    "\n",
    "details = {\n",
    "    \n",
    "    'text' : X,\n",
    "    'headlines' : Y,\n",
    "    'keywords' : Z,\n",
    "    'summary_art' : S1,\n",
    "    'entities' : E,\n",
    "    'text_ner' : text_ner,\n",
    "    'sentence_1s' : S1s,\n",
    "    'sentence_3s' : S3s,\n",
    "    'summary_ext' : S2,\n",
    "    'summary_abs' : S3,\n",
    "    'summary_extabs' : S4,\n",
    "    'summary_ner' : S5,\n",
    "    'summary_t5' : S6,\n",
    "    'predict_1s' : P0,\n",
    "    'predict_3s' : P1,\n",
    "    'predict_text' : P2,\n",
    "    'predict_ext' : P3,\n",
    "    'predict_abs' : P4,\n",
    "    'predict_extabs' : P5,\n",
    "    'predict_ner' : P6,\n",
    "    'predict_t5' : P7\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# size = len(X)\n",
    "\n",
    "# S2 = [\"\"] * size\n",
    "# S3 = [\"\"] * size\n",
    "# S4 = [\"\"] * size\n",
    "# S5 = [\"\"] * size\n",
    "# P1 = [\"\"] * size\n",
    "# P2 = [\"\"] * size\n",
    "# P3 = [\"\"] * size\n",
    "# P4 = [\"\"] * size\n",
    "# P5 = [\"\"] * size\n",
    "\n",
    "# details = {\n",
    "#     'text' : X,\n",
    "#     'headlines' : Y,\n",
    "#     'keywords' : Z,\n",
    "#     'summary_art' : S1,\n",
    "#     'summary_ext' : S2,\n",
    "#     'summary_abs' : S3,\n",
    "#     'summary_extabs' : S4,\n",
    "#     'summary_ner' : S5,\n",
    "#     'predict_text' : P1,\n",
    "#     'predict_ext' : P2,\n",
    "#     'predict_abs' : P3,\n",
    "#     'predict_extabs' : P4,\n",
    "#     'predict_ner' : P5\n",
    "# }\n",
    "\n",
    "df_test = pd.DataFrame(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://pypi.org/project/bert-extractive-summarizer/\n",
    "# # https://arxiv.org/abs/1908.10084\n",
    "\n",
    "df = df_test\n",
    "\n",
    "#abs_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)\n",
    "#abs_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "ext_model = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def summarize_text(row):\n",
    "    \n",
    "    text = row[1]['text_ner']\n",
    "    headlines = row[1]['headlines']\n",
    "    keywords = row[1]['keywords']\n",
    "    entities = row[1]['entities']\n",
    "    sentence_1s = row[1]['sentence_1s']\n",
    "    sentence_3s = row[1]['sentence_3s']\n",
    "   \n",
    "    summary_art = row[1]['summary_art']\n",
    "    summary_ext = row[1]['summary_ext']\n",
    "    summary_abs = row[1]['summary_abs']\n",
    "    summary_extabs = row[1]['summary_extabs']\n",
    "    summary_ner = row[1]['summary_ner']\n",
    "    summary_t5 = row[1]['summary_t5']\n",
    "    max_len = min(len(summary_abs), 280)\n",
    "    max_sentence_percent = 0.5\n",
    "    \n",
    "    try:\n",
    "        #ext_model = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
    "        result = ext_model(text, num_sentences=10)\n",
    "        summary_ext = ''.join(result)\n",
    "        summary_extabs = abs_summarizer(summary_ext, max_length=min(len(summary_ext), 280), min_length=15, do_sample=False)[0]['summary_text']\n",
    "        #summary_ner = summary_ners(text, entities, max_sentence_percent)\n",
    "        summary_abs = abs_summarizer(text, max_length=min(len(text), 280), min_length=15, do_sample=False)[0]['summary_text']\n",
    "    except Exception as error:\n",
    "        summary_abs = abs_summarizer(text[:2048], max_length=min(len(text), 280), min_length=15, do_sample=False)[0]['summary_text']\n",
    "        return text, headlines, keywords, entities, sentence_1s, sentence_3s, summary_art, summary_ext, summary_abs,  summary_extabs, summary_ner, summary_t5\n",
    "    return text, headlines, keywords, entities, sentence_1s, sentence_3s, summary_art, summary_ext, summary_abs,  summary_extabs, summary_ner, summary_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ner = []\n",
    "headlines = []\n",
    "keywords = []\n",
    "entities = []\n",
    "sentence_1s = []\n",
    "sentence_3s = []\n",
    "summary_art = []\n",
    "summary_ext = []\n",
    "summary_abs = []\n",
    "summary_extabs = []\n",
    "summary_ner = []\n",
    "summary_t5 = []\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "start = time.time()\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "S = Parallel(n_jobs=4)(delayed(summarize_text)(row) for row in df.iterrows())\n",
    "\n",
    "print(\"Response time (mins): \", round((time.time() - start)/60, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = list(filter(None, S))\n",
    "for i in range(len(S)):  \n",
    "        text_ner.append(S[i][0])  # text\n",
    "        headlines.append(S[i][1])  # headlines\n",
    "        keywords.append(S[i][2])  # keywords\n",
    "        entities.append(S[i][3])  # entities\n",
    "        sentence_1s.append(S[i][4])  # sentence_1s\n",
    "        sentence_3s.append(S[i][5])  # sentence_3s\n",
    "        summary_art.append(S[i][6])  # summary_art\n",
    "        summary_ext.append(S[i][7])  # summary_ext\n",
    "        summary_abs.append(S[i][8])  # summary_abs\n",
    "        summary_extabs.append(S[i][9])  # summary_extabs\n",
    "        summary_ner.append(S[i][10])  # summary_ner\n",
    "        summary_t5.append(S[i][11])  # summary_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = details_dict['text']\n",
    "# Y = details_dict['headlines']\n",
    "# Z = details_dict['keywords']\n",
    "# S1 = details_dict['summary_art']\n",
    "# S2 = details_dict['summary_ext']\n",
    "# S3 = details_dict['summary_abs']\n",
    "# S4 = details_dict['summary_extabs']\n",
    "# S5 = details_dict['summary_ner']\n",
    "# P1 = details_dict['predict_base']\n",
    "# P2 = details_dict['predict_ext']\n",
    "# P3 = details_dict['predict_abs']\n",
    "# P4 = details_dict['predict_extabs']\n",
    "# P5 = details_dict['predict_ner']\n",
    "\n",
    "\n",
    "#X = details_dict['text']\n",
    "#Y = details_dict['headlines']\n",
    "#Z = details_dict['keywords']\n",
    "# S1 = W\n",
    "# S2 = V\n",
    "# S3 = U\n",
    "# S4 = [''] * size\n",
    "#S5 = [''] * size\n",
    "\n",
    "P0 = [''] * size\n",
    "P1 = [''] * size\n",
    "P2 = [''] * size\n",
    "P3 = [''] * size\n",
    "P4 = [''] * size\n",
    "P5 = [''] * size\n",
    "P6 = [''] * size\n",
    "P7 = [''] * size\n",
    "\n",
    "details = {\n",
    "    'headlines' : headlines,\n",
    "    'text' : text,\n",
    "    'keywords' : keywords,\n",
    "    'summary_art' : summary_art,\n",
    "    'entities' : entities,\n",
    "    'text_ner' : text_ner,\n",
    "    'sentence_1s' : sentence_1s,\n",
    "    'sentence_3s' : sentence_3s,\n",
    "    'summary_ext' : summary_ext,\n",
    "    'summary_abs' : summary_abs,\n",
    "    'summary_extabs' : summary_extabs,\n",
    "    'summary_ner' : summary_ner,\n",
    "    'summary_t5' : summary_t5,\n",
    "    'predict_1s' : P0,\n",
    "    'predict_3s' : P1,\n",
    "    'predict_text' : P2,\n",
    "    'predict_ext' : P3,\n",
    "    'predict_abs' : P4,\n",
    "    'predict_extabs' : P5,\n",
    "    'predict_ner' : P6,\n",
    "    'predict_t5' : P7\n",
    "}\n",
    "\n",
    "\n",
    "# details = {\n",
    "#     'headlines' : Y,\n",
    "#     'text' : X,\n",
    "#     'keywords' : Z,\n",
    "#     'summary_art' : S1,\n",
    "#     'entities' : E,\n",
    "#     'sentence_1s' : S1s,\n",
    "#     'sentence_3s' : S3s,\n",
    "#     'summary_ext' : S2,\n",
    "#     'summary_abs' : S3,\n",
    "#     'summary_extabs' : S4,\n",
    "#     'summary_ner' : S5,\n",
    "#     'summary_t5' : summary_t5,\n",
    "#     'predict_1s' : P0,\n",
    "#     'predict_3s' : P1,\n",
    "#     'predict_text' : P2,\n",
    "#     'predict_ext' : P3,\n",
    "#     'predict_abs' : P4,\n",
    "#     'predict_extabs' : P5,\n",
    "#     'predict_ner' : P6,\n",
    "#     'predict_t5' : P7\n",
    "# }\n",
    "\n",
    "\n",
    "# details = {\n",
    "#     'text' : text,\n",
    "#     'headlines' : headlines,\n",
    "#     'keywords' : keywords,\n",
    "#     'summary_art' : summary_art,\n",
    "#     'summary_ext' : summary_ext,\n",
    "#     'summary_abs' : summary_abs,\n",
    "#     'summary_extabs' : summary_extabs,\n",
    "#     'summary_ner' : S5,\n",
    "#     'predict_base' : P1,\n",
    "#     'predict_ext' : P2,\n",
    "#     'predict_abs' : P3,\n",
    "#     'predict_extabs' : P4,\n",
    "#     'predict_ner' : P5\n",
    "# }\n",
    "\n",
    "#df_test = pd.DataFrame(details)\n",
    "\n",
    "jsonString = json.dumps(details)\n",
    "jsonFile = open(os.path.join(path_processed_data, 'cnn_test_summary_ner_v3.json'), \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = {k: details[k] for k in ('headlines', 'summary_art', 'summary_ext', 'summary_abs')}\n",
    "df_results = {k: details[k] for k in ('summary_ext', 'summary_abs', 'summary_extabs', 'summary_art', 'summary_t5')}\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(HTML(df_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results = {k: details[k] for k in ('headlines', 'summary_art', 'summary_ext', 'summary_abs')}\n",
    "df_results = {k: details[k] for k in ('sentence_1s', 'sentence_3s', 'text')}\n",
    "df_results = pd.DataFrame(df_results)\n",
    "display(HTML(df_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NB5_v1_w266_FP_Summary_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
